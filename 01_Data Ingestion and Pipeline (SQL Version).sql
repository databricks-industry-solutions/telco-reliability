-- Databricks notebook source
-- MAGIC %md
-- MAGIC ## Telecommunications Reliability Metrics
-- MAGIC
-- MAGIC **Telecommunications LTE Architecture**
-- MAGIC <br>
-- MAGIC
-- MAGIC
-- MAGIC
-- MAGIC The modern telecommunications network consists of the Base Station also known as the **eNodeB (Evolved Node B) for 4G networks** is the hardware that communicates directly with the **UE (User Enitity such as a Mobile Phone)**. The **MME (Mobility Management Entity)** manages the entire process from a cell phone making a connection to a network to a paging message being sent to the mobile phone.  
-- MAGIC <img style="margin: auto" src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/Telco_simple.png" width="1200"/>
-- MAGIC
-- MAGIC **Use Case Overview**
-- MAGIC * Telecommunications services collect many different forms of data to observe overall network reliability as well as to predict how best to expand the network to reach more customers. Some typical types of data collected are:
-- MAGIC   - **PCMD (Per Call Measurement Data):** granular details of all network processes as MME (Mobility Management Entity) manages processes between the UE and the rest of the network
-- MAGIC   - **CDR (Call Detail Records):** high level data describing call and SMS activity with fields such as phone number origin, phone number target, status of call/sms, duration, etc. 
-- MAGIC * This data can be collected and used in provide a full view of the health of each cell tower in the network as well as the network as a whole. 
-- MAGIC * **Note:** for this demo we will be primarily focused on CDR data but will also have a small sample of what PCMD could look like.
-- MAGIC
-- MAGIC **Business Impact of Solution**
-- MAGIC * **Ease of Scaling:** with large amounts of data being generated by a telecommunications system, Databricks can provide the ability to scale so that the data can be reliably ingested and analyzed.  
-- MAGIC * **Greater Network Reliability:** with the ability to monitor and predict dropped communications and more generally network faults, telecommunications providers can ultimately deliver better service for their customers and reduce churn.
-- MAGIC
-- MAGIC **Full Architecture from Ingestion to Analytics and Machine Learning**
-- MAGIC
-- MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/telco_pipeline_full.png" width="1000"/>
-- MAGIC
-- MAGIC
-- MAGIC   
-- MAGIC

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## Data Ingestion with Delta Live Tables
-- MAGIC
-- MAGIC To simplify the ingestion process and accelerate our developments, we'll leverage Delta Live Table (DLT).
-- MAGIC
-- MAGIC DLT let you declare your transformations and will handle the Data Engineering complexity for you:
-- MAGIC - Data quality tracking with expectations
-- MAGIC - Continuous or scheduled ingestion, orchestrated as pipeline
-- MAGIC - Build lineage and manage data dependencies
-- MAGIC - Automating scaling and fault tolerance
-- MAGIC
-- MAGIC **Bronze Layer** 
-- MAGIC
-- MAGIC * Ingestion here starts with loading CDR and PCMD data directly from S3 using Autoloader. Though in this example JSON files are loaded into S3 from where Autoloader will then ingest these files into the bronze layer, streams from Kafka, Kinesis, etc. are supported by simply changing the "format" option on the read operation.
-- MAGIC
-- MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/telco_pipeline_bronze.png
-- MAGIC " width="1000"/>

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE cdr_stream_bronze
COMMENT "CDR Stream - Bronze"
AS SELECT *
  FROM cloud_files(
    "s3a://db-gtm-industry-solutions/data/CME/telco/CDR",
    "json",
    map(
      "header", "false",
      "mergeSchema", "true",
      "cloudFiles.inferColumnTypes", "true"
    )
  )


-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE pcmd_stream_bronze
COMMENT "PCMD Stream - Bronze"
AS SELECT *
  FROM cloud_files(
    "s3a://db-gtm-industry-solutions/data/CME/telco/PCMD",
    "json",
    map(
      "header", "false",
      "mergeSchema", "true",
      "cloudFiles.inferColumnTypes", "true"
    )
  )

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## Joining with Tower Data and Creating the Silver Layer 
-- MAGIC
-- MAGIC **Silver Layer**
-- MAGIC
-- MAGIC * In the silver layer, the data is refined removing nulls and duplicates while also joining tower information such as state, longitude, and latitude to allow for geospatial analysis. Stream-static joins are performed to do this with the streaming CDR and PCMD records being joined with static tower information which has been stored previously.
-- MAGIC
-- MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/telco_pipeline_silver.png" width="1000"/>

-- COMMAND ----------

CREATE TEMPORARY LIVE VIEW static_tower_data AS
SELECT 
  properties.GlobalID as GlobalID,
  properties.LocCity as City,
  properties.LocCounty as County,
  properties.LocState as State,
  geometry.coordinates[1] as Longitude,
  geometry.coordinates[0] as Latitude
FROM json.`s3a://db-gtm-industry-solutions/data/CME/telco/cell_towers.json.gz`

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE cdr_stream_silver (
  CONSTRAINT towerId_not_null EXPECT (towerId IS NOT NULL) ON VIOLATION DROP ROW, 
  CONSTRAINT type_not_null EXPECT (type IS NOT NULL) ON VIOLATION DROP ROW
) COMMENT "CDR Stream - Silver (Tower Info Added)" AS
SELECT
  *
from
  stream(LIVE.cdr_stream_bronze)
  JOIN LIVE.static_tower_data ON cdr_stream_bronze.towerId = static_tower_data.GlobalID

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE pcmd_stream_silver (
  CONSTRAINT towerId_not_null EXPECT (towerId IS NOT NULL) ON VIOLATION DROP ROW, 
  CONSTRAINT ProcedureId_not_null EXPECT (ProcedureId IS NOT NULL) ON VIOLATION DROP ROW,
  CONSTRAINT ProcedureDuration_gt_0 EXPECT (ProcedureDuration > 0)
) COMMENT "PCMD Stream - Silver (Tower Info Added)" AS
SELECT
  *
from
  stream(LIVE.pcmd_stream_bronze)
  JOIN LIVE.static_tower_data ON pcmd_stream_bronze.towerId = static_tower_data.GlobalID

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## Aggregating on Various Time Periods to Create the Gold Layer
-- MAGIC With Spark **Structured Streaming** the streaming records can be automatically aggregated with stateful processing. Here the aggregation is done on 1 minute intervals and the KPIs are aggregated accordingly. Any interval can be selected here and larger time window aggregations can be done on a scheduled basis with Databricks **Workflows**. For example, the records that are aggregated here at 1 minute intervals can then be aggregated to hour long intervals with a workflow that runs every hour. 
-- MAGIC
-- MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/telco_pipeline_gold.png" width="1000"/>

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE cdr_stream_minute_gold AS
SELECT
  first(window.start) as date,
  towerId,
  count(CASE WHEN status = "answered" THEN 1 END) AS answered,
  count(CASE WHEN status = "dropped" THEN 1 END) AS dropped,
  count(CASE WHEN status = "missed" THEN 1 END) AS missed,
  count(CASE WHEN type = "call" THEN 1 END) AS call,
  count(CASE WHEN type = "text" THEN 1 END) AS text,
  count(*) AS totalRecords_CDR,
  first(Latitude) as Latitude,
  first(Longitude) as Longitude,
  first(City) as City,
  first(County) as County,
  first(State) as State
FROM
  stream(LIVE.cdr_stream_silver)
GROUP BY
  window(event_ts, "1 minute"),
  towerId


-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE pcmd_stream_minute_gold AS
SELECT
  first(window.start) as date,
  towerId,
  AVG(CASE WHEN ProcedureId = "11" THEN ProcedureDuration END) AS avg_dur_request_to_release_bearer,
  AVG(CASE WHEN ProcedureId = "15" THEN ProcedureDuration END) AS avg_dur_notification_data_sent_to_UE,
  AVG(CASE WHEN ProcedureId = "16" THEN ProcedureDuration END) AS avg_dur_request_to_setup_bearer,
  MAX(CASE WHEN ProcedureId = "11" THEN ProcedureDuration END) AS max_dur_request_to_release_bearer,
  MAX(CASE WHEN ProcedureId = "15" THEN ProcedureDuration END) AS max_dur_notification_data_sent_to_UE,
  MAX(CASE WHEN ProcedureId = "16" THEN ProcedureDuration END) AS max_dur_request_to_setup_bearer,
  MIN(CASE WHEN ProcedureId = "11" THEN ProcedureDuration END) AS min_dur_request_to_release_bearer,
  MIN(CASE WHEN ProcedureId = "15" THEN ProcedureDuration END) AS min_dur_notification_data_sent_to_UE,
  MIN(CASE WHEN ProcedureId = "16" THEN ProcedureDuration END) AS min_dur_request_to_setup_bearer,
  count(*) AS totalRecords_PCMD,
  first(Latitude) as Latitude,
  first(Longitude) as Longitude,
  first(City) as City,
  first(County) as County,
  first(State) as State
FROM
  stream(LIVE.pcmd_stream_silver)
GROUP BY
  window(event_ts, "1 minute"),
  towerId

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## Aggregating on Larger Time Windows Through Scheduled Batch Workflows
-- MAGIC As a last step in this data pipeline, hourly and daily aggregations of tower KPIs will be created as seen in the steps below. This process has been included in this Delta Live Tables pipeline for illustrative purposes but would typically be run on a batch hourly or daily basis in a real world scenario.
-- MAGIC
-- MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/telco_pipeline_batch.png" width="1000"/>

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE cdr_stream_hour_gold AS
SELECT
  first(window.start) as date,
  towerId,
  sum(answered) AS answered,
  sum(dropped) as dropped,
  sum(missed) AS missed,
  sum(call) AS call,
  sum(text) AS text,
  count(*) AS totalRecords_CDR,
  first(Latitude) as Latitude,
  first(Longitude) as Longitude,
  first(City) as City,
  first(County) as County,
  first(State) as State
FROM
  stream(LIVE.cdr_stream_minute_gold)
GROUP BY
  window(date, "1 hour"),
  towerId

-- COMMAND ----------

CREATE OR REFRESH STREAMING LIVE TABLE cdr_stream_day_gold AS
SELECT
  first(window.start) as date,
  towerId,
  sum(answered) AS answered,
  sum(dropped) as dropped,
  sum(missed) AS missed,
  sum(call) AS call,
  sum(text) AS text,
  count(*) AS totalRecords_CDR,
  first(Latitude) as Latitude,
  first(Longitude) as Longitude,
  first(City) as City,
  first(County) as County,
  first(State) as State
FROM
  stream(LIVE.cdr_stream_minute_gold)
GROUP BY
  window(date, "1 day"),
  towerId

-- COMMAND ----------

-- MAGIC %md 
-- MAGIC ## Using Databricks SQL for Analytics and Reliability Monitoring
-- MAGIC Once the gold tables of different time aggregations are in place, analysis can be done in the Databricks SQL persona view. We provide a sample dashboard which shows the overall health of the network starting with the total number of calls and dropped calls, a geospatial view to analyze the problem areas, and lastly a table to see the find the details of every tower. 
-- MAGIC
-- MAGIC <br>
-- MAGIC <br>
-- MAGIC
-- MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/Telcodashboard.png" width="1000"/>
